{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to learn with quantum neural networks\n",
    "==============================================\n",
    "\n",
    "::: {.meta}\n",
    ":property=\\\"og:description\\\": Use a classical recurrent neural network\n",
    "to initilize the parameters of a variational quatum algorithm.\n",
    ":property=\\\"og:image\\\": ../demonstrations/learning2learn/thumbnail.png\n",
    ":::\n",
    "\n",
    "::: {.related}\n",
    "tutorial\\_qaoa\\_intro Intro to QAOA tutorial\\_qaoa\\_maxcut QAOA for\n",
    "MaxCut problem\n",
    ":::\n",
    "\n",
    "*Author: Stefano Mangini --- Posted: 02 March 2021. Last updated: 15\n",
    "September 2021.*\n",
    "\n",
    "In this demo we recreate the architecture proposed in *Learning to learn\n",
    "with quantum neural networks via classical neural networks*, using\n",
    "**PennyLane** and **TensorFlow**. We use classical recurrent neural\n",
    "networks to assist the optimization of variational quantum algorithms.\n",
    "\n",
    "We start with a brief theoretical overview explaining the problem and\n",
    "the setup used to solve it. After that, we deep dive into the code to\n",
    "build a fully functioning model, ready to be further developed or\n",
    "customized for your own needs. Without further ado, let's begin!\n",
    "\n",
    "Problem: Optimization of Variational Quantum Algorithms\n",
    "-------------------------------------------------------\n",
    "\n",
    "Recently, a big effort by the quantum computing community has been\n",
    "devoted to the study of variational quantum algorithms (VQAs) which\n",
    "leverage quantum circuits with fixed shape and tunable parameters. The\n",
    "idea is similar to classical neural networks, where the weights of the\n",
    "network are optimized during training. Similarly, once the shape of the\n",
    "variational quantum circuit is chosen --- something that is very\n",
    "difficult and sensitive to the particular task at hand --- its tunable\n",
    "parameters are optimized iteratively by minimizing a cost (or loss)\n",
    "function, which measures how good the quantum algorithm is performing\n",
    "(see for a thorough overview on VQAs).\n",
    "\n",
    "A major challenge for VQAs relates to the optimization of tunable\n",
    "parameters, which was shown to be a very hard task, . Parameter\n",
    "initialization plays a key role in this scenario, since initializing the\n",
    "parameters in the proximity of an optimal solution leads to faster\n",
    "convergence and better results. Thus, a good initialization strategy is\n",
    "crucial to promote the convergence of local optimizers to local extrema\n",
    "and to select reasonably good local minima. By local optimizer, we mean\n",
    "a procedure that moves from one solution to another by small (local)\n",
    "changes in parameter space. These are opposed to global search methods,\n",
    "which take into account large sections of parameter space to propose a\n",
    "new solution.\n",
    "\n",
    "One such strategy could come from the classical machine learning\n",
    "literature.\n",
    "\n",
    "Solution: Classical Recurrent Neural Networks\n",
    "---------------------------------------------\n",
    "\n",
    "By building on results from the *meta-learning* literature in machine\n",
    "learning, authors in propose to use a Recurrent Neural Network (RNN) as\n",
    "a black-box controller to optimize the parameters of variational quantum\n",
    "algorithms, as shown in the figure below. The cost function used is the\n",
    "expectation value\n",
    "$\\langle H \\rangle_{\\boldsymbol{\\theta}} = \\langle \\psi_{\\boldsymbol{\\theta}} | H | \\psi_{\\boldsymbol{\\theta}}\\rangle$\n",
    "of a Hamiltonian $H$ with respect to the parametrized state\n",
    "$|\\psi_\\boldsymbol{\\theta}\\rangle$ evolved by applying the variational\n",
    "quantum circuit to the zero state $|00\\cdots0\\rangle$.\n",
    "\n",
    "![](../demonstrations/learning2learn/HybridLSTM.png){.align-center\n",
    "width=\"100.0%\"}\n",
    "\n",
    "Given parameters $\\boldsymbol{\\theta}_{t-1}$ of the variational quantum\n",
    "circuit, the cost function $y_{t-1}$, and the hidden state of the\n",
    "classical network $\\boldsymbol{h}_{t-1}$ at the previous time step, the\n",
    "recurrent neural network proposes a new guess for the parameters\n",
    "$\\boldsymbol{\\theta}_t$, which are then fed into the quantum computer to\n",
    "evaluate the cost function $y_t$. By repeating this cycle a few times,\n",
    "and by training the weights of the recurrent neural network to minimize\n",
    "the loss function $y_t$, a good initialization heuristic is found for\n",
    "the parameters $\\boldsymbol{\\theta}$ of the variational quantum circuit.\n",
    "\n",
    "At a given iteration, the RNN receives as input the previous cost\n",
    "function $y_t$ evaluated on the quantum computer, where $y_t$ is the\n",
    "estimate of $\\langle H\\rangle_{t}$, as well as the parameters\n",
    "$\\boldsymbol{\\theta}_t$ for which the variational circuit was evaluated.\n",
    "The RNN at this time step also receives information stored in its\n",
    "internal hidden state from the previous time step $\\boldsymbol{h}_t$.\n",
    "The RNN itself has trainable parameters $\\phi$, and hence it applies the\n",
    "parametrized mapping:\n",
    "\n",
    "$$\\boldsymbol{h}_{t+1}, \\boldsymbol{\\theta}_{t+1} = \\text{RNN}_{\\phi}(\\boldsymbol{h}_{t}, \\boldsymbol{\\theta}_{t}, y_{t}),$$\n",
    "\n",
    "which generates a new suggestion for the variational parameters as well\n",
    "as a new internal state. Upon training the weights $\\phi$, the RNN\n",
    "eventually learns a good heuristic to suggest optimal parameters for the\n",
    "quantum circuit.\n",
    "\n",
    "Thus, by training on a dataset of graphs, the RNN can subsequently be\n",
    "used to provide suggestions for starting points on new graphs! We are\n",
    "not directly optimizing the variational parameters of the quantum\n",
    "circuit, but instead, we let the RNN figure out how to do that. In this\n",
    "sense, we are learning (training the RNN) how to learn (how to optimize\n",
    "a variational quantum circuit).\n",
    "\n",
    "**VQAs in focus: QAOA for MaxCut**\n",
    "\n",
    "There are multiple VQAs for which this hybrid training routine could be\n",
    "used, some of them directly analyzed in. In the following, we focus on\n",
    "one such example, the Quantum Approximate Optimization Algorithm (QAOA)\n",
    "for solving the MaxCut problem. Thus, referring to the picture above,\n",
    "the shape of the variational circuit is the one dictated by the QAOA\n",
    "ansatz, and such a quantum circuit is used to evaluate the cost\n",
    "Hamiltonian $H$ of the MaxCut problem. Check out this great tutorial on\n",
    "how to use QAOA for solving graph problems:\n",
    "<https://pennylane.ai/qml/demos/tutorial_qaoa_intro.html>\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "Running the tutorial (excluding the Appendix) requires approx. \\~13m.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the required packages**\n",
    "\n",
    "During this tutorial, we will use **PennyLane** for executing quantum\n",
    "circuits and for integrating seamlessly with **TensorFlow**, which will\n",
    "be used for creating the RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Machine Learning\n",
    "import pennylane as qml\n",
    "from pennylane import qaoa\n",
    "\n",
    "# Classical Machine Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generation of graphs\n",
    "import networkx as nx\n",
    "\n",
    "# Standard Python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Fix the seed for reproducibility, which affects all random functions in this demo\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of training data: graphs\n",
    "===================================\n",
    "\n",
    "The first step is to gather or create a good dataset that will be used\n",
    "to train the model and test its performance. In our case, we are\n",
    "analyzing MaxCut, which deals with the problem of finding a good binary\n",
    "partition of nodes in a graph such that the number of edges *cut* by\n",
    "such a separation is maximized. We start by generating some random\n",
    "graphs $G_{n,p}$ where:\n",
    "\n",
    "-   $n$ is the number of nodes in each graph,\n",
    "-   $p$ is the probability of having an edge between two nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphs(n_graphs, n_nodes, p_edge):\n",
    "    \"\"\"Generate a list containing random graphs generated by Networkx.\"\"\"\n",
    "\n",
    "    datapoints = []\n",
    "    for _ in range(n_graphs):\n",
    "        random_graph = nx.gnp_random_graph(n_nodes, p=p_edge)\n",
    "        datapoints.append(random_graph)\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a random graph generated using the function\n",
    "`generate_graphs` just defined:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define parameters of the graphs\n",
    "n_graphs = 20\n",
    "n_nodes = 7\n",
    "p_edge = 3.0 / n_nodes\n",
    "graphs = generate_graphs(n_graphs, n_nodes, p_edge)\n",
    "\n",
    "nx.draw(graphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../demonstrations/learning2learn/rendered_Graph0.png){.align-center\n",
    "width=\"70.0%\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Quantum Circuit: QAOA\n",
    "=================================\n",
    "\n",
    "Now that we have a dataset, we move on by creating the QAOA quantum\n",
    "circuits using PennyLane's built-in sub-packages. In particular, using\n",
    "PennyLane's `qaoa` module, we will able to create fully functioning\n",
    "quantum circuits for the MaxCut problem, with very few lines of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaoa_from_graph(graph, n_layers=1):\n",
    "    \"\"\"Uses QAOA to create a cost Hamiltonian for the MaxCut problem.\"\"\"\n",
    "\n",
    "    # Number of qubits (wires) equal to the number of nodes in the graph\n",
    "    wires = range(len(graph.nodes))\n",
    "\n",
    "    # Define the structure of the cost and mixer subcircuits for the MaxCut problem\n",
    "    cost_h, mixer_h = qaoa.maxcut(graph)\n",
    "\n",
    "    # Defines a layer of the QAOA ansatz from the cost and mixer Hamiltonians\n",
    "    def qaoa_layer(gamma, alpha):\n",
    "        qaoa.cost_layer(gamma, cost_h)\n",
    "        qaoa.mixer_layer(alpha, mixer_h)\n",
    "\n",
    "    # Creates the actual quantum circuit for the QAOA algorithm\n",
    "    def circuit(params, **kwargs):\n",
    "        for w in wires:\n",
    "            qml.Hadamard(wires=w)\n",
    "        qml.layer(qaoa_layer, n_layers, params[0], params[1])\n",
    "        return qml.expval(cost_h)\n",
    "\n",
    "    # Evaluates the cost Hamiltonian\n",
    "    def hamiltonian(params, **kwargs):\n",
    "        \"\"\"Evaluate the cost Hamiltonian, given the angles and the graph.\"\"\"\n",
    "\n",
    "        # We set the default.qubit.tf device for seamless integration with TensorFlow\n",
    "        dev = qml.device(\"default.qubit.tf\", wires=len(graph.nodes))\n",
    "\n",
    "        # This qnode evaluates the expectation value of the cost hamiltonian operator\n",
    "        cost = qml.QNode(circuit, dev, interface=\"tf\", diff_method=\"backprop\")\n",
    "\n",
    "        return cost(params)\n",
    "\n",
    "    return hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's see how to use these functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a QAOA circuit given a graph.\n",
    "cost = qaoa_from_graph(graph=graphs[0], n_layers=1)\n",
    "\n",
    "# Since we use only one layer in QAOA, params have the shape 1 x 2,\n",
    "# in the form [[alpha, gamma]].\n",
    "x = tf.Variable([[0.5], [0.5]], dtype=tf.float32)\n",
    "\n",
    "# Evaluate th QAOA instance just created with some angles.\n",
    "print(cost(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.rst-class}\n",
    "sphx-glr-script-out\n",
    "\n",
    "Out:\n",
    "\n",
    "``` {.none}\n",
    "tf.Tensor(-3.193267957255582, shape=(), dtype=float64)\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network: LSTM\n",
    "==============================\n",
    "\n",
    "So far, we have defined the machinery which lets us build the QAOA\n",
    "algorithm for solving the MaxCut problem. Now we wish to implement the\n",
    "Recurrent Neural Network architecture explained previously. As proposed\n",
    "in the original paper, we will build a custom model of a Long-Short Term\n",
    "Memory (LSTM) network, capable of handling the hybrid data passing\n",
    "between classical and quantum procedures. For this task, we will use\n",
    "`Keras` and `TensorFlow`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's define the elemental building block of the model, an\n",
    "LSTM cell (see [TensorFlow\n",
    "documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell)\n",
    "for further details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of layers in the QAOA ansatz.\n",
    "# The higher the better in terms of performance, but it also gets more\n",
    "# computationally expensive. For simplicity, we stick to the single layer case.\n",
    "n_layers = 1\n",
    "\n",
    "# Define a single LSTM cell.\n",
    "# The cell has two units per layer since each layer in the QAOA ansatz\n",
    "# makes use of two parameters.\n",
    "cell = tf.keras.layers.LSTMCell(2 * n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `qaoa_from_graph` function, we create a list `graph_cost_list`\n",
    "containing the cost functions of a set of graphs. You can see this as a\n",
    "preprocessing step of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the QAOA MaxCut cost functions of some graphs\n",
    "graph_cost_list = [qaoa_from_graph(g) for g in graphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we seek to reproduce the recurrent behavior depicted in\n",
    "the picture above, outlining the functioning of an RNN as a black-box\n",
    "optimizer. We do so by defining two functions:\n",
    "\n",
    "-   `rnn_iteration`: accounts for the computations happening on a single\n",
    "    time step in the figure. It performs the calculation inside the CPU\n",
    "    and evaluates the quantum circuit on the QPU to obtain the loss\n",
    "    function for the current parameters.\n",
    "-   `recurrent_loop`: as the name suggests, it accounts for the creation\n",
    "    of the recurrent loop of the model. In particular, it makes\n",
    "    consecutive calls to the `rnn_iteration` function, where the outputs\n",
    "    of a previous call are fed as inputs of the next call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_iteration(inputs, graph_cost, n_layers=1):\n",
    "    \"\"\"Perform a single time step in the computational graph of the custom RNN.\"\"\"\n",
    "\n",
    "    # Unpack the input list containing the previous cost, parameters,\n",
    "    # and hidden states (denoted as 'h' and 'c').\n",
    "    prev_cost = inputs[0]\n",
    "    prev_params = inputs[1]\n",
    "    prev_h = inputs[2]\n",
    "    prev_c = inputs[3]\n",
    "\n",
    "    # Concatenate the previous parameters and previous cost to create new input\n",
    "    new_input = tf.keras.layers.concatenate([prev_cost, prev_params])\n",
    "\n",
    "    # Call the LSTM cell, which outputs new values for the parameters along\n",
    "    # with new internal states h and c\n",
    "    new_params, [new_h, new_c] = cell(new_input, states=[prev_h, prev_c])\n",
    "\n",
    "    # Reshape the parameters to correctly match those expected by PennyLane\n",
    "    _params = tf.reshape(new_params, shape=(2, n_layers))\n",
    "\n",
    "    # Evaluate the cost using new angles\n",
    "    _cost = graph_cost(_params)\n",
    "\n",
    "    # Reshape to be consistent with other tensors\n",
    "    new_cost = tf.reshape(tf.cast(_cost, dtype=tf.float32), shape=(1, 1))\n",
    "\n",
    "    return [new_cost, new_params, new_h, new_c]\n",
    "\n",
    "\n",
    "def recurrent_loop(graph_cost, n_layers=1, intermediate_steps=False):\n",
    "    \"\"\"Creates the recurrent loop for the Recurrent Neural Network.\"\"\"\n",
    "\n",
    "    # Initialize starting all inputs (cost, parameters, hidden states) as zeros.\n",
    "    initial_cost = tf.zeros(shape=(1, 1))\n",
    "    initial_params = tf.zeros(shape=(1, 2 * n_layers))\n",
    "    initial_h = tf.zeros(shape=(1, 2 * n_layers))\n",
    "    initial_c = tf.zeros(shape=(1, 2 * n_layers))\n",
    "\n",
    "    # We perform five consecutive calls to 'rnn_iteration', thus creating the\n",
    "    # recurrent loop. More iterations lead to better results, at the cost of\n",
    "    # more computationally intensive simulations.\n",
    "    out0 = rnn_iteration([initial_cost, initial_params, initial_h, initial_c], graph_cost)\n",
    "    out1 = rnn_iteration(out0, graph_cost)\n",
    "    out2 = rnn_iteration(out1, graph_cost)\n",
    "    out3 = rnn_iteration(out2, graph_cost)\n",
    "    out4 = rnn_iteration(out3, graph_cost)\n",
    "\n",
    "    # This cost function takes into account the cost from all iterations,\n",
    "    # but using different weights.\n",
    "    loss = tf.keras.layers.average(\n",
    "        [0.1 * out0[0], 0.2 * out1[0], 0.3 * out2[0], 0.4 * out3[0], 0.5 * out4[0]]\n",
    "    )\n",
    "\n",
    "    if intermediate_steps:\n",
    "        return [out0[1], out1[1], out2[1], out3[1], out4[1], loss]\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cost function**\n",
    "\n",
    "A key part in the `recurrent_loop` function is given by the definition\n",
    "of the variable `loss`. In order to drive the learning procedure of the\n",
    "weights in the LSTM cell, a cost function is needed. While in the\n",
    "original paper the authors suggest using a measure called *observed\n",
    "improvement*, for simplicity here we use an easier cost function\n",
    "$\\cal{L}(\\phi)$ defined as:\n",
    "\n",
    "$$\\cal{L}(\\phi) = {\\bf w} \\cdot {\\bf y}_t(\\phi),$$\n",
    "\n",
    "where ${\\bf y}_t(\\phi) = (y_1, \\cdots, y_5)$ contains the Hamiltonian\n",
    "cost functions from all iterations, and ${\\bf w}$ are just some\n",
    "coefficients weighting the different steps in the recurrent loop. In\n",
    "this case, we used ${\\bf w}=\\frac{1}{5} (0.1, 0.2, 0.3, 0.4, 0.5)$, to\n",
    "give more importance to the last steps rather than the initial steps.\n",
    "Intuitively in this way the RNN is more free (low coefficient) to\n",
    "explore a larger portion of parameter space during the first steps of\n",
    "optimization, while it is constrained (high coefficient) to select an\n",
    "optimal solution towards the end of the procedure. Note that one could\n",
    "also use just the final cost function from the last iteration to drive\n",
    "the training procedure of the RNN. However, using values also from\n",
    "intermediate steps allows for a smoother suggestion routine, since even\n",
    "non-optimal parameter suggestions from early steps are penalized using\n",
    "$\\cal{L}(\\phi)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "Now all the cards are on the table and we just need to prepare a\n",
    "training routine and then run it!\n",
    "\n",
    "First of all, let's wrap a single gradient descent step inside a custom\n",
    "function `train_step`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(graph_cost):\n",
    "    \"\"\"Single optimization step in the training procedure.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Evaluates the cost function\n",
    "        loss = recurrent_loop(graph_cost)\n",
    "\n",
    "    # Evaluates gradients, cell is the LSTM cell defined previously\n",
    "    grads = tape.gradient(loss, cell.trainable_weights)\n",
    "\n",
    "    # Apply gradients and update the weights of the LSTM cell\n",
    "    opt.apply_gradients(zip(grads, cell.trainable_weights))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the training. In particular, we will perform a\n",
    "stochastic gradient descent in the parameter space of the weights of the\n",
    "LSTM cell. For each graph in the training set, we evaluate gradients and\n",
    "update the weights accordingly. Then, we repeat this procedure for\n",
    "multiple times (epochs).\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "Be careful when using bigger datasets or training for larger epochs,\n",
    "this may take a while to execute.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    total_loss = np.array([])\n",
    "    for i, graph_cost in enumerate(graph_cost_list):\n",
    "        loss = train_step(graph_cost)\n",
    "        total_loss = np.append(total_loss, loss.numpy())\n",
    "        # Log every 5 batches.\n",
    "        if i % 5 == 0:\n",
    "            print(f\" > Graph {i+1}/{len(graph_cost_list)} - Loss: {loss[0][0]}\")\n",
    "    print(f\" >> Mean Loss during epoch: {np.mean(total_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.rst-class}\n",
    "sphx-glr-script-out\n",
    "\n",
    "Out:\n",
    "\n",
    "``` {.none}\n",
    "Epoch 1\n",
    " > Graph 1/20 - Loss: -1.6641689538955688\n",
    " > Graph 6/20 - Loss: -1.4186843633651733\n",
    " > Graph 11/20 - Loss: -1.3757232427597046\n",
    " > Graph 16/20 - Loss: -1.294339656829834\n",
    " >> Mean Loss during epoch: -1.7352586269378663\n",
    "Epoch 2\n",
    " > Graph 1/20 - Loss: -2.119091749191284\n",
    " > Graph 6/20 - Loss: -1.4789190292358398\n",
    " > Graph 11/20 - Loss: -1.3779840469360352\n",
    " > Graph 16/20 - Loss: -1.2963457107543945\n",
    " >> Mean Loss during epoch: -1.8252217948436738\n",
    "Epoch 3\n",
    " > Graph 1/20 - Loss: -2.1322619915008545\n",
    " > Graph 6/20 - Loss: -1.459418535232544\n",
    " > Graph 11/20 - Loss: -1.390620470046997\n",
    " > Graph 16/20 - Loss: -1.3165746927261353\n",
    " >> Mean Loss during epoch: -1.8328069806098939\n",
    "Epoch 4\n",
    " > Graph 1/20 - Loss: -2.1432175636291504\n",
    " > Graph 6/20 - Loss: -1.476362943649292\n",
    " > Graph 11/20 - Loss: -1.3938289880752563\n",
    " > Graph 16/20 - Loss: -1.3140206336975098\n",
    " >> Mean Loss during epoch: -1.8369774043560028\n",
    "Epoch 5\n",
    " > Graph 1/20 - Loss: -2.1429405212402344\n",
    " > Graph 6/20 - Loss: -1.477513074874878\n",
    " > Graph 11/20 - Loss: -1.3909202814102173\n",
    " > Graph 16/20 - Loss: -1.315887689590454\n",
    " >> Mean Loss during epoch: -1.8371947884559632\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Loss for each graph keeps decreasing across epochs,\n",
    "indicating that the training routine is working correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "=======\n",
    "\n",
    "Let's see how to use the optimized RNN as an initializer for the angles\n",
    "in the QAOA algorithm.\n",
    "\n",
    "First, we pick a new graph, not present in the training dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_graph = nx.gnp_random_graph(7, p=3 / 7)\n",
    "new_cost = qaoa_from_graph(new_graph)\n",
    "\n",
    "nx.draw(new_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../demonstrations/learning2learn/rendered_Graph1.png){.align-center\n",
    "width=\"70.0%\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the trained RNN to this new graph, saving intermediate\n",
    "results coming from all the recurrent iterations in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the RNN (be sure that training was performed)\n",
    "res = recurrent_loop(new_cost, intermediate_steps=True)\n",
    "\n",
    "# Extract all angle suggestions\n",
    "start_zeros = tf.zeros(shape=(2 * n_layers, 1))\n",
    "guess_0 = res[0]\n",
    "guess_1 = res[1]\n",
    "guess_2 = res[2]\n",
    "guess_3 = res[3]\n",
    "guess_4 = res[4]\n",
    "final_loss = res[5]\n",
    "\n",
    "# Wrap them into a list\n",
    "guesses = [start_zeros, guess_0, guess_1, guess_2, guess_3, guess_4]\n",
    "\n",
    "# Losses from the hybrid LSTM model\n",
    "lstm_losses = [new_cost(tf.reshape(guess, shape=(2, n_layers))) for guess in guesses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot of the loss function**\n",
    "\n",
    "We can plot these losses to see how well the RNN proposes new guesses\n",
    "for the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(lstm_losses, color=\"blue\", lw=3, ls=\"-.\", label=\"LSTM\")\n",
    "\n",
    "plt.grid(ls=\"--\", lw=2, alpha=0.25)\n",
    "plt.ylabel(\"Cost function\", fontsize=12)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.legend()\n",
    "ax.set_xticks([0, 5, 10, 15, 20]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../demonstrations/learning2learn/rendered_LossLSTM.png){.align-center\n",
    "width=\"70.0%\"}\n",
    "\n",
    "That's remarkable! The RNN learned to propose new parameters such that\n",
    "the MaxCut cost is minimized very rapidly: in just a few iterations the\n",
    "loss reaches a minimum. Actually, it takes just a single step for the\n",
    "LSTM to find a very good minimum. In fact, due to the recurrent loop,\n",
    "the loss in each time step is directly dependent on the previous ones,\n",
    "with the first iteration thus having a lot of influence on the loss\n",
    "function defined above. Changing the loss function, for example giving\n",
    "less importance to initial steps and just focusing on the last one,\n",
    "leads to different optimization behaviors, but with the same final\n",
    "results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with standard Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "How well does this method compare with standard optimization techniques,\n",
    "for example, leveraging Stochastic Gradient Descent (SGD) to optimize\n",
    "the parameters in the QAOA?\n",
    "\n",
    "Let's check it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are randomly initialized\n",
    "x = tf.Variable(np.random.rand(2, 1))\n",
    "\n",
    "# We set the optimizer to be a Stochastic Gradient Descent\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "step = 15\n",
    "\n",
    "# Training process\n",
    "steps = []\n",
    "sdg_losses = []\n",
    "for _ in range(step):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = new_cost(x)\n",
    "\n",
    "    steps.append(x)\n",
    "    sdg_losses.append(loss)\n",
    "\n",
    "    gradients = tape.gradient(loss, [x])\n",
    "    opt.apply_gradients(zip(gradients, [x]))\n",
    "    print(f\"Step {_+1} - Loss = {loss}\")\n",
    "\n",
    "print(f\"Final cost function: {new_cost(x).numpy()}\\nOptimized angles: {x.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.rst-class}\n",
    "sphx-glr-script-out\n",
    "\n",
    "Out:\n",
    "\n",
    "``` {.none}\n",
    "Step 1 - Loss = -4.1700805\n",
    "Step 2 - Loss = -4.67503588\n",
    "Step 3 - Loss = -5.09949909\n",
    "Step 4 - Loss = -5.40388533\n",
    "Step 5 - Loss = -5.59529203\n",
    "Step 6 - Loss = -5.70495197\n",
    "Step 7 - Loss = -5.7642561\n",
    "Step 8 - Loss = -5.79533198\n",
    "Step 9 - Loss = -5.81138752\n",
    "Step 10 - Loss = -5.81966529\n",
    "Step 11 - Loss = -5.82396722\n",
    "Step 12 - Loss = -5.82624537\n",
    "Step 13 - Loss = -5.82749126\n",
    "Step 14 - Loss = -5.82820626\n",
    "Step 15 - Loss = -5.82864379\n",
    "Final cost function: -5.828932361904984\n",
    "Optimized angles: [[ 0.5865477 ]\n",
    " [-0.3228858]]\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(sdg_losses, color=\"orange\", lw=3, label=\"SGD\")\n",
    "\n",
    "plt.plot(lstm_losses, color=\"blue\", lw=3, ls=\"-.\", label=\"LSTM\")\n",
    "\n",
    "plt.grid(ls=\"--\", lw=2, alpha=0.25)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cost function\", fontsize=12)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "ax.set_xticks([0, 5, 10, 15, 20]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../demonstrations/learning2learn/rendered_LossConfrontation.png){.align-center\n",
    "width=\"70.0%\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hurray!* \n",
    "\n",
    "As is clear from the picture, the RNN reaches a better minimum in fewer\n",
    "iterations than the standard SGD. Thus, as the authors suggest, the\n",
    "trained RNN can be used for a few iterations at the start of the\n",
    "training procedure to initialize the parameters of the quantum circuit\n",
    "close to an optimal solution. Then, a standard optimizer like the SGD\n",
    "can be used to fine-tune the proposed parameters and reach even better\n",
    "solutions. While on this small scale example the benefits of using an\n",
    "LSTM to initialize parameters may seem modest, on more complicated\n",
    "instances and problems it can make a big difference, since, on random\n",
    "initialization of the parameters, standard local optimizer may encounter\n",
    "problems finding a good minimization direction (for further details,\n",
    "see,).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final remarks\n",
    "=============\n",
    "\n",
    "In this demo, we saw how to use a recurrent neural network as a\n",
    "black-box optimizer to initialize the parameters in a variational\n",
    "quantum circuit close to an optimal solution. We connected MaxCut QAOA\n",
    "quantum circuits in PennyLane with an LSTM built with TensorFlow, and we\n",
    "used a custom hybrid training routine to optimize the whole network.\n",
    "\n",
    "Such architecture proved itself to be a good candidate for the\n",
    "initialization problem of Variational Quantum Algorithms, since it\n",
    "reaches good optimal solutions in very few iterations. Besides, the\n",
    "architecture is quite general since the same machinery can be used for\n",
    "graphs having a generic number of nodes (see \\\"Generalization\n",
    "Performances\\\" in the Appendix).\n",
    "\n",
    "**What's next?**\n",
    "\n",
    "But the story does not end here. There are multiple ways this work could\n",
    "be improved. Here are a few:\n",
    "\n",
    "-   Use the proposed architecture for VQAs other than QAOA for MaxCut.\n",
    "    You can check the paper to get some inspiration.\n",
    "-   Scale up the simulation, using bigger graphs and longer recurrent\n",
    "    loops.\n",
    "-   While working correctly, the training routine is quite basic and it\n",
    "    could be improved for example by implementing batch learning or a\n",
    "    stopping criterion. Also, one could implement the *observed\n",
    "    improvement* loss function, as used in the original paper .\n",
    "-   Depending on the problem, you may wish to transform the functions\n",
    "    `rnn_iteration` and `recurrent_loop` to actual `Keras Layers` and\n",
    "    `Models`. This way, by compiling the model before the training takes\n",
    "    place, `TensorFlow` can create the computational graph of the model\n",
    "    and train more efficiently. You can find some ideas below to start\n",
    "    working on it.\n",
    "\n",
    "If you\\'re interested, in the Appendix below you can find some more\n",
    "details and insights about this model. Go check it out!\n",
    "\n",
    "If you have any doubt, or wish to discuss about the project don't\n",
    "hesitate to contact me, I'll be very happy to help you as much as I can\n",
    "\n",
    "\n",
    "Have a great quantum day!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "==========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix\n",
    "========\n",
    "\n",
    "In this appendix you can find further details about the Learning to\n",
    "Learn approach introduced in this tutorial.\n",
    "\n",
    "Generalization performances\n",
    "---------------------------\n",
    "\n",
    "A very interesting feature of this model, is that it can be\n",
    "straightforwardly applied to graphs having a different number of nodes.\n",
    "In fact, until now our analysis focused only on graphs with the same\n",
    "number of nodes for ease of explanation, and there is no actual\n",
    "restriction in this respect. The same machinery works fine for any\n",
    "graph, since the number of QAOA parameters are only dependent on the\n",
    "number of layers in the ansatz, and not on the number of qubits (equal\n",
    "to the number of nodes in the graph) in the quantum circuit.\n",
    "\n",
    "Thus, we might want to challenge our model to learn a good\n",
    "initialization heuristic for a non-specific graph, with an arbitrary\n",
    "number of nodes. For this purpose, let's create a training dataset\n",
    "containing graphs with a different number of nodes $n$, taken in the\n",
    "interval $n \\in [7,9]$ (that is, our dataset now contains graphs having\n",
    "either 7, 8 and 9 nodes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.keras.layers.LSTMCell(2 * n_layers)\n",
    "\n",
    "g7 = generate_graphs(5, 7, 3 / 7)\n",
    "g8 = generate_graphs(5, 8, 3 / 7)\n",
    "g9 = generate_graphs(5, 9, 3 / 7)\n",
    "\n",
    "gs = g7 + g8 + g9\n",
    "gs_cost_list = [qaoa_from_graph(g) for g in gs]\n",
    "\n",
    "# Shuffle the dataset\n",
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(gs_cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have created an equally balanced dataset that contains graphs\n",
    "with a different number of nodes. We now use this dataset to train the\n",
    "LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    total_loss = np.array([])\n",
    "    for i, graph_cost in enumerate(gs_cost_list):\n",
    "        loss = train_step(graph_cost)\n",
    "        total_loss = np.append(total_loss, loss.numpy())\n",
    "        # Log every 5 batches.\n",
    "        if i % 5 == 0:\n",
    "            print(f\" > Graph {i+1}/{len(gs_cost_list)} - Loss: {loss}\")\n",
    "    print(f\" >> Mean Loss during epoch: {np.mean(total_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.rst-class}\n",
    "sphx-glr-script-out\n",
    "\n",
    "Out:\n",
    "\n",
    "``` {.none}\n",
    "Epoch 1\n",
    "> Graph 1/15 - Loss: [[-1.4876363]]\n",
    "> Graph 6/15 - Loss: [[-1.8590403]]\n",
    "> Graph 11/15 - Loss: [[-1.7644017]]\n",
    ">> Mean Loss during epoch: -1.9704322338104248\n",
    "Epoch 2\n",
    "> Graph 1/15 - Loss: [[-1.8650053]]\n",
    "> Graph 6/15 - Loss: [[-1.9578737]]\n",
    "> Graph 11/15 - Loss: [[-1.8377447]]\n",
    ">> Mean Loss during epoch: -2.092947308222453\n",
    "Epoch 3\n",
    "> Graph 1/15 - Loss: [[-1.9009062]]\n",
    "> Graph 6/15 - Loss: [[-1.9726204]]\n",
    "> Graph 11/15 - Loss: [[-1.8668792]]\n",
    ">> Mean Loss during epoch: -2.1162660201390584\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if this hybrid model eventually learned a good heuristic to\n",
    "propose new updates for the parameters in the QAOA ansatz of the MaxCut\n",
    "problem.\n",
    "\n",
    "For this reason, we consider a new graph. In particular, we can take a\n",
    "graph with 10 nodes, which is something that the recurrent network has\n",
    "not seen before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_graph = nx.gnp_random_graph(10, p=3 / 7)\n",
    "new_cost = qaoa_from_graph(new_graph)\n",
    "\n",
    "nx.draw(new_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../demonstrations/learning2learn/rendered_Graph10.png){.align-center\n",
    "width=\"70.0%\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the trained recurrent LSTM on this graph, saving not only the\n",
    "last, but all intermediate guesses for the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = recurrent_loop(new_cost, intermediate_steps=True)\n",
    "\n",
    "# Extract all angle suggestions\n",
    "start_zeros = tf.zeros(shape=(2 * n_layers, 1))\n",
    "guess_0 = res[0]\n",
    "guess_1 = res[1]\n",
    "guess_2 = res[2]\n",
    "guess_3 = res[3]\n",
    "guess_4 = res[4]\n",
    "final_loss = res[5]\n",
    "\n",
    "# Wrap them into a list\n",
    "guesses = [start_zeros, guess_0, guess_1, guess_2, guess_3, guess_4]\n",
    "\n",
    "# Losses from the hybrid LSTM model\n",
    "lstm_losses = [new_cost(tf.reshape(guess, shape=(2, n_layers))) for guess in guesses]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(lstm_losses, color=\"blue\", lw=3, ls=\"-.\", label=\"LSTM\")\n",
    "\n",
    "plt.grid(ls=\"--\", lw=2, alpha=0.25)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cost function\", fontsize=12)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "ax.set_xticks([0, 5, 10, 15, 20]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../demonstrations/learning2learn/rendered_LossGeneralization.png){.align-center\n",
    "width=\"70.0%\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can confirm that the custom optimizer based on the LSTM\n",
    "quickly reaches a good value of the loss function, and also achieve good\n",
    "generalization performances, since it is able to initialize parameters\n",
    "also for graphs not present in the training set.\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "To get the optimized weights of the LSTM use:\n",
    "`optimized_weights = cell.get_weights()`. To set initial weights for the\n",
    "LSTM cell, use instead: `cell.set_weights(optimized_weights)`.\n",
    ":::\n",
    "\n",
    "Loss landscape in parameter space\n",
    "=================================\n",
    "\n",
    "It may be interesting to plot the path suggested by the RNN in the space\n",
    "of the parameters. Note that this is possible only if one layer is used\n",
    "in the QAOA ansatz since in this case only two angles are needed and\n",
    "they can be plotted on a 2D plane. Of course, if more layers are used,\n",
    "you can always select a pair of them to reproduce a similar plot.\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "This cell takes approx. \\~1m to run with an 11 by 11 grid\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the cost function on a grid in parameter space\n",
    "dx = dy = np.linspace(-1.0, 1.0, 11)\n",
    "dz = np.array([new_cost([[xx], [yy]]).numpy() for yy in dy for xx in dx])\n",
    "Z = dz.reshape((11, 11))\n",
    "\n",
    "# Plot cost landscape\n",
    "plt.contourf(dx, dy, Z)\n",
    "plt.colorbar()\n",
    "\n",
    "# Extract optimizer steps\n",
    "params_x = [0.0] + [res[i].numpy()[0, 0] for i in range(len(res[:-1]))]\n",
    "params_y = [0.0] + [res[i].numpy()[0, 1] for i in range(len(res[:-1]))]\n",
    "\n",
    "# Plot steps\n",
    "plt.plot(params_x, params_y, linestyle=\"--\", color=\"red\", marker=\"x\")\n",
    "\n",
    "plt.yticks(np.linspace(-1, 1, 5))\n",
    "plt.xticks(np.linspace(-1, 1, 5))\n",
    "plt.xlabel(r\"$\\alpha$\", fontsize=12)\n",
    "plt.ylabel(r\"$\\gamma$\", fontsize=12)\n",
    "plt.title(\"Loss Landscape\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../demonstrations/learning2learn/rendered_LossLandscape.png){.align-center\n",
    "width=\"70.0%\"}\n",
    "\n",
    "Ideas for creating a Keras Layer and Keras Model\n",
    "================================================\n",
    "\n",
    "Definition of a `Keras Layer` containing a single pass through the LSTM\n",
    "and the Quantum Circuit. That's equivalent to the function\n",
    "`rnn_iteration` from before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, p=1, graph=None):\n",
    "        super(QRNN, self).__init__()\n",
    "        # p is the number of layers in the QAOA ansatz\n",
    "        self.cell = tf.keras.layers.LSTMCell(2 * p)\n",
    "        self.expectation = qaoa_from_graph(graph, n_layers=p)\n",
    "        self.qaoa_p = p\n",
    "\n",
    "    def call(self, inputs):\n",
    "        prev_cost = inputs[0]\n",
    "        prev_params = inputs[1]\n",
    "        prev_h = inputs[2]\n",
    "        prev_c = inputs[3]\n",
    "\n",
    "        # Concatenate the previous parameters and previous cost to create new input\n",
    "        new_input = tf.keras.layers.concatenate([prev_cost, prev_params])\n",
    "\n",
    "        # New parameters obtained by the LSTM cell, along with new internal states h and c\n",
    "        new_params, [new_h, new_c] = self.cell(new_input, states=[prev_h, prev_c])\n",
    "\n",
    "        # This part is used to feed the parameters to the PennyLane function\n",
    "        _params = tf.reshape(new_params, shape=(2, self.qaoa_p))\n",
    "\n",
    "        # Cost evaluation, and reshaping to be consistent with other Keras tensors\n",
    "        new_cost = tf.reshape(tf.cast(self.expectation(_params), dtype=tf.float32), shape=(1, 1))\n",
    "\n",
    "        return [new_cost, new_params, new_h, new_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for creating an actual `Keras Model` starting from the previous\n",
    "layer definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_graph = nx.gnp_random_graph(7, p=3 / 7)\n",
    "\n",
    "# Instantiate the LSTM cells\n",
    "rnn0 = QRNN(graph=_graph)\n",
    "\n",
    "# Create some input layers to feed the data\n",
    "inp_cost = tf.keras.layers.Input(shape=(1,))\n",
    "inp_params = tf.keras.layers.Input(shape=(2,))\n",
    "inp_h = tf.keras.layers.Input(shape=(2,))\n",
    "inp_c = tf.keras.layers.Input(shape=(2,))\n",
    "\n",
    "# Manually creating the recurrent loops. In this case just three iterations are used.\n",
    "out0 = rnn0([inp_cost, inp_params, inp_h, inp_c])\n",
    "out1 = rnn0(out0)\n",
    "out2 = rnn0(out1)\n",
    "\n",
    "# Definition of a loss function driving the training of the LSTM\n",
    "loss = tf.keras.layers.average([0.15 * out0[0], 0.35 * out1[0], 0.5 * out2[0]])\n",
    "\n",
    "# Definition of a Keras Model\n",
    "model = tf.keras.Model(\n",
    "    inputs=[inp_cost, inp_params, inp_h, inp_c], outputs=[out0[1], out1[1], out2[1], loss]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.rst-class}\n",
    "sphx-glr-script-out\n",
    "\n",
    "Out:\n",
    "\n",
    "``` {.none}\n",
    "Model: \"functional_1\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_3 (InputLayer)            [(None, 2)]          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_4 (InputLayer)            [(None, 2)]          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "qrnn (QRNN)                     [(1, 1),             48         input_1[0][0]                    \n",
    "                                 (None, 2),                     input_2[0][0]                    \n",
    "                                 (None, 2),                     input_3[0][0]                    \n",
    "                                 (None, 2)]                     input_4[0][0]                    \n",
    "                                                                qrnn[0][0]                       \n",
    "                                                                qrnn[0][1]                       \n",
    "                                                                qrnn[0][2]                       \n",
    "                                                                qrnn[0][3]                       \n",
    "                                                                qrnn[1][0]                       \n",
    "                                                                qrnn[1][1]                       \n",
    "                                                                qrnn[1][2]                       \n",
    "                                                                qrnn[1][3]                       \n",
    "__________________________________________________________________________________________________\n",
    "tf.math.multiply (TFOpLambda)   (1, 1)               0           qrnn[0][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "tf.math.multiply_1 (TFOpLambda) (1, 1)               0           qrnn[1][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "tf.math.multiply_2 (TFOpLambda) (1, 1)               0           qrnn[2][0]                       \n",
    "__________________________________________________________________________________________________\n",
    "average_147 (Average)           (1, 1)               0           tf.math.multiply[0][0]           \n",
    "                                                                tf.math.multiply_1[0][0]         \n",
    "                                                                tf.math.multiply_2[0][0]         \n",
    "==================================================================================================\n",
    "Total params: 48\n",
    "Trainable params: 48\n",
    "Non-trainable params: 0\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic training routine for the `Keras Model` just created:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "\n",
    "inp_costA = tf.zeros(shape=(1, 1))\n",
    "inp_paramsA = tf.zeros(shape=(1, 2 * p))\n",
    "inp_hA = tf.zeros(shape=(1, 2 * p))\n",
    "inp_cA = tf.zeros(shape=(1, 2 * p))\n",
    "\n",
    "inputs = [inp_costA, inp_paramsA, inp_hA, inp_cA]\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "step = 5\n",
    "\n",
    "for _ in range(step):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(inputs)\n",
    "        loss = pred[3]\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(\n",
    "        f\"Step {_+1} - Loss = {loss} - Cost = {qaoa_from_graph(_graph, n_layers=p)(np.reshape(pred[2].numpy(),(2, p)))}\"\n",
    "    )\n",
    "\n",
    "print(\"Final Loss:\", loss.numpy())\n",
    "print(\"Final Outs:\")\n",
    "for t, s in zip(pred, [\"out0\", \"out1\", \"out2\", \"Loss\"]):\n",
    "    print(f\" >{s}: {t.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.rst-class}\n",
    "sphx-glr-script-out\n",
    "\n",
    "Out:\n",
    "\n",
    "``` {.none}\n",
    "Step 1 - Loss = [[-1.5563084]] - Cost = -4.762684301954701\n",
    "Step 2 - Loss = [[-1.5649065]] - Cost = -4.799981173473755\n",
    "Step 3 - Loss = [[-1.5741502]] - Cost = -4.840036354736862\n",
    "Step 4 - Loss = [[-1.5841404]] - Cost = -4.883246647056216\n",
    "Step 5 - Loss = [[-1.5948243]] - Cost = -4.929228976649736\n",
    "Final Loss: [[-1.5948243]]\n",
    "Final Outs:\n",
    ">out0: [[-0.01041588  0.01016874]]\n",
    ">out1: [[-0.04530389  0.38148248]]\n",
    ">out2: [[-0.10258182  0.4134117 ]]\n",
    ">Loss: [[-1.5948243]]\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "This code works only for a single graph at a time, since a graph was\n",
    "needed to create the `QRNN` `Keras Layer` named `rnn0`. Thus, in order\n",
    "to actually train the RNN network for multiple graphs, the above\n",
    "training routine must be modified. Otherwise, you could find a way to\n",
    "define the model to accept as input a whole dataset of graphs, and not\n",
    "just a single one. Still, this might prove particularly hard, since\n",
    "TensorFlow deals with tensors, and is not able to directly manage other\n",
    "data structures, like graphs or functions taking graphs as input, like\n",
    "`qaoa_from_graph`.\n",
    ":::\n",
    "\n",
    "About the author\n",
    "================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
